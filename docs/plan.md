# Высоконагруженная система управления и балансировки сервисов

## **Краткое описание**

Это распределенная информационно-аналитическая система, предназначенная для централизованного управления парком веб-сервисов, развернутых в Docker-контейнерах. Система реализует функции балансировки нагрузки, динамической конфигурации обратного прокси, автоматического масштабирования, мониторинга и обеспечения безопасности на основе архитектуры "основной узел (Core) — дочерние узлы (Agent)". Управление всеми аспектами системы осуществляется через единый веб-интерфейс.

## **Архитектура и философия**

Система строится на двух ключевых компонентах:

1.  **Core (Ядро/Основной узел):** Мозг всей системы. Это "панель управления".
    *   **Backend (Go, Chi):** Предоставляет REST API для веб-интерфейса и для взаимодействия с агентами. Отвечает за аутентификацию (JWT), хранение конфигураций и метрик в PostgreSQL, бизнес-логику (когда и какие команды отдать агентам), а также за отправку уведомлений.
    *   **Frontend (React):** Единая точка входа для пользователя. Предоставляет интерфейсы для управления всеми модулями системы.
    *   **NGINX (Core):** Выполняет две роли:
        1.  Роутинг UI/API: запросы на `/api/*` перенаправляет на бэкенд, все остальные — на фронтенд.
        2.  **Глобальный балансировщик (опционально, но рекомендуется):** Этот NGINX может служить единой точкой входа для всего пользовательского трафика, который он затем перенаправляет на NGINX соответствующего агента. Это упрощает управление DNS (все домены указывают на IP Core-узла).

2.  **Agent (Агент/Дочерний узел):** "Руки" системы. Устанавливается на каждой хост-машине, где работают сервисы.
    *   **Go App (Agent):** Легковесное приложение, которое периодически (например, раз в 1-5 секунд) обращается к Core API.
        *   **На исходящий запрос (к Core):** Отправляет собранные метрики (CPU, RAM, disk, network хоста, статистика по контейнерам), статусы сервисов и логи.
        *   **На входящий ответ (от Core):** Получает массив задач (например: "создать конфиг для a.com", "запустить контейнер acme.sh", "заблокировать IP x.x.x.x").
    *   **NGINX (Agent):** Локальный обратный прокси. Конфигурация этого NGINX динамически управляется приложением-агентом. Он направляет трафик на конкретные контейнеры на данном хосте.
    *   **Docker Engine:** Агент взаимодействует с Docker Engine хост-машины через сокет (`/var/run/docker.sock`) для управления контейнерами (запуск, остановка, репликация, получение логов и статистики).

**Ключевой аспект:** Для того чтобы Docker-контейнер с агентом мог собирать метрики хостовой системы, его нужно запускать с монтированием системных директорий и сокета Docker:
`docker run -v /:/host:ro -v /var/run/docker.sock:/var/run/docker.sock ...`
Флаг `:ro` (read-only) для корневой файловой системы повышает безопасность. Чтение файлов из `/host/proc`, `/host/sys` позволит получить метрики хоста.

---

## **Детальное описание компонентов (Модулей)**

1.  **Модуль конфигурирования балансировщика**
    *   **Core:** В UI пользователь создает/редактирует "Сервис", указывая домен (`app.example.com`), целевой узел (Agent), порт контейнера и т.д. Эта конфигурация сохраняется в PostgreSQL.
    *   **Agent:** При очередном запросе к Core, агент получает задачу "обновить конфигурацию NGINX". Он генерирует `.conf` файл в `/etc/nginx/conf.d/` и выполняет `nginx -s reload` (через Docker, используя `os/exec`), что позволяет применить изменения без прерывания соединений.

2.  **Модуль авто-масштабирования**
    *   **Core:** Анализирует метрики (CPU/RAM контейнера, RPS), получаемые от агента. Если показатели превышают заданный порог (например, CPU > 80% в течение 1 минуты), Core добавляет в очередь задач для агента команду на запуск еще одной реплики контейнера (`docker run ...` или `docker-compose up -d --scale service_name=N`).
    *   **Agent:** Выполняет команду от Core для изменения числа реплик. Обновляет NGINX-конфигурацию (секцию `upstream`), чтобы включить новый контейнер в балансировку.

3.  **Модуль управления DNS-записями**
    *   Это не управление публичным DNS, а управление `server_name` в NGINX. Реализация идентична модулю 1. Пользователь в UI привязывает домен к узлу, Core передает эту информацию агенту, а агент создает соответствующий `server { ... }` блок в конфигурации NGINX.

4.  **Модуль автоматизации SSL**
    *   **Core:** В UI пользователь нажимает "Выпустить SSL" для домена. Core формирует задачу для агента.
    *   **Agent:** Получив задачу, запускает контейнер `neilpang/acme.sh` с нужными параметрами (домен, email, метод верификации http-01). Для http-01 верификации агент временно меняет конфиг NGINX, чтобы Let's Encrypt мог достучаться до нужного файла. После успешного получения сертификатов, агент обновляет основной конфиг NGINX для этого домена, включая пути к сертификатам и настраивая редирект с HTTP на HTTPS.

5.  **Модуль шаблонов веб-страниц (Custom Error Pages)**
    *   **Core:** В UI есть редактор для создания HTML-шаблонов страниц (404, 502, 503 и т.д.). Шаблоны хранятся в PostgreSQL.
    *   **Agent:** При получении задачи "обновить шаблоны ошибок", агент скачивает их с Core API и сохраняет на диск. В конфигурации NGINX для каждого сайта прописываются директивы `error_page`, указывающие на эти файлы.

6.  **Модуль анализа трафика и изоляции (WAF - Web Application Firewall)**
    *   **Agent:** Это самый сложный модуль для реализации "с нуля".
        *   **Сбор данных:** Агент парсит логи доступа NGINX (`access.log`) в реальном времени.
        *   **Простой анализ (Rate Limiting):** В памяти агент хранит счетчики запросов по IP-адресам. При превышении порога (например, 100 запросов в секунду) агент генерирует правило для NGINX (`deny <IP-адрес>;`) и добавляет его в специальный файл, который подключен в `nginx.conf`. Затем — `nginx -s reload`.
        *   **Сложный анализ:** Агент может отправлять агрегированную статистику по запросам (URL, user-agent, статус-коды) на Core для более глубокого анализа.
    *   **Core:** Получает данные от агентов. Может применять более сложные правила (например, "если с одного IP идут запросы к `wp-login.php` с 10 разных сайтов на разных агентах — это атака, заблокировать IP везде"). Core отдает команду на блокировку всем агентам.

7.  **Модуль мониторинга и аналитики**
    *   **Agent:**
        *   **Хост:** Читает `/host/proc/stat` (CPU), `/host/proc/meminfo` (RAM), `/host/proc/diskstats` (диск), `/host/proc/net/dev` (сеть).
        *   **Контейнеры:** Использует Docker Engine API (через сокет) для получения статистики по каждому контейнеру (`docker stats` по сути делает то же самое).
        *   **Кастомные метрики:** Сервис внутри контейнера может предоставлять метрики по эндпоинту `/metrics` в формате JSON. Агент будет периодически опрашивать этот эндпоинт.
        *   Все собранные данные отправляются на Core.
    *   **Core:**
        *   **Backend:** Принимает метрики от агента и складывает их в PostgreSQL. Возможно, стоит использовать TimescaleDB (расширение для PostgreSQL) для эффективного хранения временных рядов.
        *   **Frontend:** Строит графики и дашборды, используя полученные данные.

8.  **Модуль централизованной обработки уведомлений**
    *   **Core:**
        *   **Backend:** Имеет логику для триггеров (например, "если CPU агента > 90% в течение 5 минут" или "сервис app.com вернул 502 статус"). В настройках пользователь указывает свой Telegram Chat ID и токен бота. Core форматирует сообщение по шаблону и отправляет его через Telegram Bot API.
        *   **Frontend:** Интерфейс для настройки правил нотификации и шаблонов сообщений.

9.  **Модуль получения и визуализации логов**
    *   **Agent:** Использует Docker Engine API для получения логов контейнера (`docker logs -f --tail=100 <container_id>`). Агент не стримит логи постоянно, а запрашивает их у Docker по команде от Core.
    *   **Core:** Когда пользователь открывает страницу логов в UI, фронтенд начинает периодически (раз в 1-2 секунды) опрашивать бэкенд. Бэкенд, в свою очередь, запрашивает свежую порцию логов у соответствующего агента. Это создает эффект "живого" лога без использования WebSocket.

---

## **Пошаговый план выполнения (от простого к сложному)**

### **Этап 0: Фундамент и подготовка**
1.  Создать репозиторий с двумя корневыми папками: `core` и `agent`.
2.  **Core:**
    *   Настроить `docker-compose.yml` для `server` (Go), `app` (React) и `postgres`.
    *   Создать базовое Go-приложение (Chi) с одним эндпоинтом `/api/ping`.
    *   Создать базовое React-приложение (Vite) на TypeScript с одной страницей.
    *   Настроить NGINX в `core` для роутинга `/api` -> `server`, `/` -> `app`.
3.  **Agent:**
    *   Создать `docker-compose.yml` для `agent` (Go) и `nginx`.
    *   Создать базовое Go-приложение, которое при старте пытается обратиться к `core:port/api/ping`.
    *   Подготовить Dockerfile для агента.

### **Этап 1: "Рукопожатие" и регистрация агентов**
1.  **Core (Backend):**
    *   Создать таблицы в PostgreSQL: `agents` (id, name, url, api_key, last_seen_at).
    *   Реализовать API для регистрации агента: `POST /api/agents/register`. При регистрации генерируется `api_key` и возвращается агенту.
    *   Реализовать защищенный эндпоинт `POST /api/heartbeat`, доступный по `api_key`. В этом эндпоинте агент будет присылать данные, а Core — возвращать задачи.
2.  **Agent:**
    *   При первом запуске без ID/KEY, агент запрашивает регистрацию у Core. Сохраняет полученный `api_key` в `.env` или на диске.
    *   Настроить периодический (каждые 5 секунд) вызов `heartbeat` к Core, отправляя свой ID.
3.  **Core (Frontend):**
    *   Создать страницу "Узлы", где будет отображаться список зарегистрированных агентов из таблицы `agents` и их статус (online/offline на основе `last_seen_at`).

### **Этап 2: Ручное управление проксированием (MVP)**
1.  **Core (Backend):**
    *   Создать таблицы `services` (id, domain, agent_id, target_port, etc.).
    *   Создать CRUD API для управления сервисами (`/api/services`).
    *   В ответе на `heartbeat` добавить поле `tasks`. Если для агента есть новая/измененная конфигурация сервиса, добавить задачу `{ "type": "UPDATE_NGINX", "payload": { ...конфигурация... } }`.
2.  **Agent:**
    *   Реализовать обработчик задачи `UPDATE_NGINX`. Он должен:
        1.  Сгенерировать `.conf` файл для NGINX на основе `payload`.
        2.  Поместить его в `/etc/nginx/conf.d/`.
        3.  Выполнить `nginx -s reload`.
3.  **Core (Frontend):**
    *   Создать форму для добавления/редактирования сервиса (домен, выбор узла из списка, порт).
    *   Отображать список всех созданных сервисов.

### **Этап 3: Базовый мониторинг**
1.  **Agent:**
    *   Реализовать функции сбора базовых метрик хоста (CPU, RAM) и статистики по всем контейнерам (через Docker сокет).
    *   Добавить эти метрики в тело запроса `heartbeat`.
2.  **Core (Backend):**
    *   Создать таблицы для хранения метрик.
    *   Расширить эндпоинт `heartbeat` для приема и сохранения метрик.
    *   Создать новые API эндпоинты для получения исторических метрик (`GET /api/agents/{id}/metrics?range=1h`).
3.  **Core (Frontend):**
    *   На странице конкретного узла добавить графики (используя, например, `Chart.js` или `ECharts`), отображающие CPU/RAM хоста в реальном времени (через периодический опрос API).
    *   Отобразить список запущенных на узле контейнеров и их текущее потребление ресурсов.

### **Этап 4: Просмотр логов**
1.  **Agent:**
    *   Реализовать обработчик задачи `{ "type": "GET_LOGS", "payload": { "container_id": "...", "tail": 100 } }`.
    *   В ответе на `heartbeat` агент будет возвращать последние N строк логов для всех контейнеров.
2.  **Core (Frontend):**
    *   На странице контейнера добавить кнопку/вкладку "Логи". При открытии фронтенд начинает опрашивать API Core и отображать логи, создавая эффект "живого стриминга".

### **Этап 5: Автоматизация SSL**
1.  **Agent:** Реализовать обработчик задачи `{ "type": "ISSUE_SSL", "payload": { "domain": "..." } }`. Обработчик должен запустить контейнер `acme.sh` и управлять конфигурацией NGINX для верификации и последующего подключения сертификатов.
2.  **Core (Backend & Frontend):** Добавить в UI для сервиса кнопку "Выпустить SSL", которая инициирует этот процесс. Отображать статус SSL (нет, в процессе, активен до...).

### **Этап 6: Расширенные функции (Автомасштабирование и WAF)**
1.  **Core:** Реализовать логику анализа метрик для автомасштабирования. В UI добавить настройки порогов.
2.  **Agent:** Реализовать обработчик задач "SCALE_UP" / "SCALE_DOWN".
3.  **Agent:** Реализовать парсинг `access.log` и базовый rate-limiting по IP.
4.  **Core & Agent:** Реализовать механизм централизованной блокировки IP.

### **Этап 7: Уведомления и кастомизация**
1.  **Core (Backend):** Реализовать систему триггеров и отправку уведомлений в Telegram.
2.  **Core (Frontend):** Создать UI для настройки правил и шаблонов уведомлений.
3.  **Agent & Core:** Реализовать логику для кастомных страниц ошибок (модуль 5).

### **Этап 8: Завершение и развертывание**
1.  Написание подробной документации для пользователя.
2.  Тестирование безопасности, производительности.
3.  Подготовка production-ready Docker-образов и инструкций по развертыванию.

Этот план позволяет двигаться итерационно, получая работающий продукт на ранних стадиях и постепенно наращивая его функциональность. Удачи в разработке